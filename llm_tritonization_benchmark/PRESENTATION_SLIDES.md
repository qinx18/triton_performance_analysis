# LLM-Driven Triton Code Generation for TSVC Benchmark

**Automated Infrastructure for GPU Kernel Generation and Validation**

---

# Part 1: Infrastructure Design

## System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    generate_and_test.py                     â”‚
â”‚                     (Main Pipeline)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â–º TSVC Function Database (151 functions)
               â”‚   â””â”€ utilities/tsvc_functions_db.py
               â”‚
               â”œâ”€â–º Static Analysis Modules (PET/ISL)
               â”‚   â”œâ”€ WAR Dependencies
               â”‚   â”œâ”€ Statement Overwrites
               â”‚   â”œâ”€ Stream Compaction
               â”‚   â”œâ”€ Loop Unrolling Patterns
               â”‚   â”œâ”€ Early Exit Detection
               â”‚   â”œâ”€ Statement Reordering
               â”‚   â”œâ”€ Scalar Expansion
               â”‚   â”œâ”€ Reduction Detection
               â”‚   â””â”€ Convolution Patterns
               â”‚
               â”œâ”€â–º LLM Generation (Claude Sonnet 4.5)
               â”‚   â”œâ”€ Initial prompt with analysis
               â”‚   â”œâ”€ Retry with error feedback (max 10)
               â”‚   â””â”€ 5+5 reset strategy
               â”‚
               â”œâ”€â–º Test Infrastructure
               â”‚   â”œâ”€ TSVC C reference (compiled shared library)
               â”‚   â”œâ”€ Triton correctness testing
               â”‚   â””â”€ Performance benchmarking vs C reference
               â”‚
               â””â”€â–º Results Collection
                   â”œâ”€ test{N}/llm_triton/  (generated code)
                   â”œâ”€ test{N}/results.json (metrics)
                   â””â”€ FINAL_TEST_RESULTS.md (analysis)
```

---

## Pipeline Flow: Per-Function Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TSVC Function   â”‚ (e.g., s421)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 1. Extract C code structure
         â”‚      - Kernel loop identification
         â”‚      - Array access patterns
         â”‚      - Local variables
         â”‚
         â”œâ”€â–º 2. Run Static Analysis
         â”‚      - PET: Dependence analysis
         â”‚      - ISL: Parallelization strategy
         â”‚      - Pattern detection (8 modules)
         â”‚
         â”œâ”€â–º 3. Build LLM Prompt
         â”‚      - C code + analysis results
         â”‚      - Triton compilation rules
         â”‚      - Function signature requirements
         â”‚
         â”œâ”€â–º 4. C Reference (Pre-compiled)
         â”‚      - Original TSVC C kernels
         â”‚      - Compiled as shared library (libtsvc_all.so)
         â”‚      - Python wrappers via ctypes
         â”‚
         â”œâ”€â–º 5. Generate Triton Code
         â”‚      â”œâ”€ Attempt 1: Initial generation
         â”‚      â”œâ”€ Attempts 2-5: Retry with errors
         â”‚      â”œâ”€ Attempt 6: Reset context
         â”‚      â””â”€ Attempts 7-10: Fresh tries
         â”‚
         â”œâ”€â–º 6. Test Correctness
         â”‚      - Compare vs TSVC C reference
         â”‚      - Multiple input sizes
         â”‚      - Tolerance: max_error < 1e-3
         â”‚
         â””â”€â–º 7. Benchmark Performance
                - 10 warmup iterations
                - 100 benchmark iterations
                - 60-second timeout per section
                - Record speedup ratio
```

---

## Key Infrastructure Components

### 1. **generate_and_test.py** (Main Pipeline)
- **Lines:** ~2,100
- **Functions:** 40+
- **Key Features:**
  - Automatic TSVC function extraction from C code
  - Integration with 8 static analysis modules
  - Retry logic with error feedback
  - Test harness auto-generation
  - Benchmark infrastructure

### 2. **TSVC Function Database**
```python
TSVC_FUNCTIONS = {
    "s421": {
        "arrays": {"a": "r", "xx": "rw", "yy": "r"},
        "has_offset": True,
        "has_conditional": False,
        "has_reduction": False,
        "category": "storage_classes"
    },
    # ... 150 more functions
}
```

### 3. **Static Analysis Modules** (PET + Custom)
| Module | Purpose | Example Output |
|--------|---------|----------------|
| `compute_war_dependences` | Detect write-after-read | "Save `a[i]` before overwrite" |
| `compute_statement_overwrites` | Detect overwrite patterns | "Use latest value only" |
| `compute_stream_compaction` | Detect if/scatter patterns | "Use atomic operations" |
| `compute_loop_unrolling` | Suggest unroll strategies | "Unroll by factor 4" |
| `compute_early_exit` | Find break conditions | "Use sequential loop" |
| `compute_statement_reordering` | RAW dependency order | "Reorder statements" |
| `compute_scalar_expansion` | Temporary variable needs | "Expand scalar to array" |
| `compute_reduction_type` | Reduction operations | "Use atomic_add" |

---

## Retry Strategy Evolution

### Initial Approach (Tests 1-16)
```
Attempt 1: Initial generation
Attempt 2-3: Retry with error
â†’ Problem: Gets stuck in same error pattern
```

### 5+5 Strategy (Test 17+)
```
Attempts 1-5:  Retry with error feedback
               â””â”€ Show last attempt + error
Attempt 6:     RESET CONTEXT
               â””â”€ Fresh generation without history
Attempts 7-10: New retry sequence
               â””â”€ Fresh perspective on the problem
```

**Result:** +3 functions passed on first try (test16â†’test17)

---

## Test Harness Auto-Generation

For each function, automatically generates:

### 1. **Correctness Test** (`my_triton_implementations/{func}/test_{func}_correctness.py`)
```python
# Auto-generated based on array specs
- Test sizes: [100, 1000, 10000] or [64, 128, 256] for 2D
- Clone tensors for isolation
- Compare outputs: max_error < 1e-3
- Return: PASS/FAIL + error details
```

### 2. **Benchmark Script** (`my_triton_implementations/{func}/benchmark_{func}.py`)
```python
# Auto-generated with timeout handling
- 10 warmup iterations (60s timeout)
- 100 benchmark iterations (60s timeout)
- Record C reference (CPU) and Triton (GPU) times
- Calculate speedup ratio
- Handle timeouts gracefully
```

---

## Prompt Engineering

### Prompt Structure (per function)
```
1. TSVC C code (30-100 lines)
2. Kernel loop to implement (5-20 lines)
3. Array information (types, sizes, access patterns)
4. Static analysis results (0-8 modules)
   â”œâ”€ WAR dependencies (if applicable)
   â”œâ”€ Statement overwrites (if applicable)
   â”œâ”€ Stream compaction (if applicable)
   â””â”€ ... other patterns
5. Function signature requirements (exact parameter names)
6. CRITICAL: Triton compilation rules (12 rules)
   â”œâ”€ NEVER use tl.arange() in loops
   â”œâ”€ NEVER use scalar indexing in kernels
   â”œâ”€ NEVER use non-existent Triton functions
   â””â”€ ... 9 more rules
7. Expected output: Python code only
```

**Total prompt size:** 500-2000 tokens (varies by complexity)

---

# Part 2: Correctness Results

## Historical Progress

*Note: Historical results were measured against LLM-generated PyTorch baseline.*
*Results with TSVC C reference baseline will be measured in new test runs.*

### Design Evolution
- Tests 1-18: PyTorch baseline (LLM-generated, potential bugs)
- Test 19+: TSVC C reference (original ground truth)

**Benefit of new design:** Removes baseline bugs, provides authoritative correctness reference.

---

## Current State (To Be Measured)

### Summary Metrics
| Metric | Count | Percentage |
|--------|-------|------------|
| âœ… **PASSING** | TBD | TBD |
| âŒ **FAILING** | TBD | TBD |
| ğŸ“Š **Benchmarked** | TBD | TBD |
| âš¡ **Valid Speedups** | TBD | TBD |
| â±ï¸ **C Ref Timeouts** | TBD | TBD |

*Note: Results will be measured against original TSVC C reference functions (ground truth).*
*Previous results were measured against LLM-generated PyTorch baseline which may have contained bugs.*

### Pass Rate by Attempt
| Attempt | New Passes | Cumulative | Rate |
|---------|------------|------------|------|
| TBD | TBD | TBD | TBD |

---

## Correctness Results (To Be Measured)

*Correctness results will be measured against original TSVC C reference functions.*

### Key Changes from Previous Design
- **Old baseline:** LLM-generated PyTorch code (potential bugs)
- **New baseline:** Original TSVC C functions (ground truth)
- **Benefit:** Removes potential baseline bugs, provides authoritative reference

### Known Issue: s421

**Error:** `ValueError: arange's arguments must be of type tl.constexpr`

**Root Cause:** LLM consistently generates incorrect kernel signature
```python
# Generated (WRONG):
@triton.jit
def s421_kernel(xx_ptr, yy_ptr, a_ptr, n):
    BLOCK_SIZE = 256                    # âŒ Regular variable
    offsets = tl.arange(0, BLOCK_SIZE)  # âŒ Compilation error

# Expected (CORRECT):
@triton.jit
def s421_kernel(xx_ptr, yy_ptr, a_ptr, n, BLOCK_SIZE: tl.constexpr):
    offsets = tl.arange(0, BLOCK_SIZE)  # âœ… Works!
```

**Recommendation:** Add explicit constexpr instruction to prompt

---

## Success by Function Category

| Category | Total | Pass | Rate | Notes |
|----------|-------|------|------|-------|
| Single dimension ops | 13 | TBD | TBD | |
| Double dimensions | 6 | TBD | TBD | |
| Induction variables | 8 | TBD | TBD | |
| Global data flow | 3 | TBD | TBD | |
| Nonlinear dependence | 2 | TBD | TBD | |
| Interprocedural | 2 | TBD | TBD | |
| Control flow | 20 | TBD | TBD | |
| Statement reordering | 4 | TBD | TBD | |
| Loop distribution | 3 | TBD | TBD | |
| Loop interchange | 6 | TBD | TBD | |
| Node splitting | 5 | TBD | TBD | |
| Scalar expansion | 6 | TBD | TBD | |
| Reductions | 13 | TBD | TBD | |
| Recurrences | 3 | TBD | TBD | |
| Search loops | 2 | TBD | TBD | |
| Packing | 3 | TBD | TBD | |
| Loop rerolling | 3 | TBD | TBD | |
| Storage classes | 4 | TBD | TBD | s421 known issue |
| Intrinsic functions | 3 | TBD | TBD | |
| Indirect addressing | 6 | TBD | TBD | |
| Vector operations | 9 | TBD | TBD | |
| Control loops | 6 | TBD | TBD | |

*Results to be measured against TSVC C reference.*

---

## Key Correctness Insights

### 1. **LLM Handles Complex Patterns Well**
- âœ… 2D loops with dependencies
- âœ… Atomic operations for scatter patterns
- âœ… Statement reordering for RAW dependencies
- âœ… Scalar expansion for temporary variables
- âœ… Conditional parallelization
- âœ… Stream compaction with cumsum

### 2. **Static Analysis is Critical**
Static analysis guidance improves LLM generation quality.

### 3. **Retry Strategy Works**
- 5+5 reset strategy helps escape error loops
- Most functions succeed within first few attempts

### 4. **Remaining Challenges**
- Implicit requirements (constexpr)
- Edge cases in prompt engineering
- LLM consistency across attempts

*Detailed statistics to be measured with C reference baseline.*

---

# Part 3: Performance Results

## Benchmark Infrastructure (Test 18)

### New Features
```
âœ… 60-second timeout per section (warmup/benchmark)
âœ… Separate timeout tracking for C reference vs Triton
âœ… Minimum speedup calculation for timeouts
âœ… Graceful error handling
âœ… Machine-readable output format
```

### Timeout Handling
```python
# C reference timeout:
- Baseline too slow (>60s for 100 iterations)
- Report: C ref time = -1ms
- Calculate minimum speedup: 60000ms / triton_time

# Triton timeout:
- Report: Triton time = -1ms

# Both timeout:
- Report: "Both timeout"
```

---

## Performance Summary

### Overall Statistics
| Metric | Value |
|--------|-------|
| **Benchmarked** | TBD |
| **Valid Speedups** | TBD |
| **C Ref Timeouts** | TBD |
| **Triton Timeouts** | TBD |
| **Average Speedup** | TBD |
| **Median Speedup** | TBD |

*Note: Performance statistics to be measured after running experiments with C reference baseline.*

### Performance Distribution
```
Functions faster than baseline:  TBD
Functions slower than baseline:  TBD
Functions with C ref timeout:    TBD
```

---

## Performance Results (To Be Measured)

Performance comparisons will be measured against the original TSVC C reference functions.

### Expected Comparison
| Comparison | Notes |
|------------|-------|
| Triton (GPU) vs C Reference (CPU) | Measures GPU acceleration benefit |
| Single kernel launch vs sequential C | Shows parallelization advantage |

*Detailed performance results will be populated after running experiments.*

---

## Top 10 Measured Speedups

| Function | Speedup | Triton (ms) | C Ref (ms) | Category |
|----------|---------|-------------|------------|----------|
| TBD | TBD | TBD | TBD | TBD |

*Speedup measurements will be populated after running experiments with C reference baseline.*

**Note:** C reference runs on CPU, Triton runs on GPU. Speedups reflect GPU parallelization benefits over sequential CPU execution.

---

## Performance by Category

*Performance by category will be measured after running experiments with C reference baseline.*

### Expected Performance Tiers
| Tier | Expected Categories | Notes |
|------|---------------------|-------|
| ğŸš€ High Speedup | Loop interchange, 2D operations | High parallelism potential |
| âš¡ Good Speedup | Most vectorizable loops | Standard GPU acceleration |
| ğŸŒ Limited Speedup | Simple operations, trivial loops | Kernel launch overhead may dominate |

---

## Why GPU Speedups?

### Understanding the Comparison

**The C Reference (CPU):**
```c
// Sequential C loop on CPU
for (int i = 0; i < 32000; i++) {
    a[i] = b[i] + 1.0;
}
```
- **Execution:** Sequential on single CPU core
- **Optimizations:** Compiler auto-vectorization (SIMD)
- **Baseline:** Represents optimized sequential C code

**The Triton Implementation (GPU):**
```python
# Single kernel launch, massively parallel
@triton.jit
def kernel(a_ptr, b_ptr, n, BLOCK_SIZE: tl.constexpr):
    # All 32000 elements in parallel across GPU cores!
```
- **Benefit:** Massive parallelism (thousands of threads)
- **Parallelism:** All elements processed simultaneously
- **Overhead:** Kernel launch + data transfer

### Comparison Context

| Comparison Type | Notes |
|-----------------|-------|
| Triton (GPU) vs C (CPU) | Measures GPU parallelization benefit |
| Triton vs Hand-optimized CUDA | ~0.5-2x (Triton generates efficient code) |

---

## Performance Insights

### 1. **What Triton Excels At**
âœ… Loop interchange patterns (high parallelism)
âœ… 2D operations with dependencies
âœ… Complex control flow
âœ… Induction variable computations
âœ… Stream compaction

### 2. **What May Not Benefit**
âŒ Trivial operations (kernel overhead may dominate)
âŒ Operations with limited parallelism
âŒ Single scalar updates (no parallelism to exploit)

### 3. **Key Observations**
- GPU parallelization provides significant speedups for vectorizable loops
- Kernel launch overhead affects small/trivial operations
- C reference provides a more realistic baseline than Python loops

*Detailed performance insights will be updated after running experiments.*

---

# Conclusions & Future Work

## Key Achievements âœ…

### Infrastructure
- âœ… Fully automated pipeline (TSVC â†’ Triton)
- âœ… 8 static analysis modules integrated
- âœ… Comprehensive test harness generation
- âœ… Retry logic with context reset
- âœ… Timeout-aware benchmarking

### Results (To Be Measured)
- â³ **Correctness rate** (vs TSVC C reference)
- â³ **First-try success rate**
- â³ **Category pass rates**
- â³ **Performance vs C reference**

---

## Limitations & Learnings

### 1. **Prompt Engineering Matters**
- Explicit > Implicit instructions
- Example code is crucial
- s421 failure: missing constexpr instruction

### 2. **Baseline Choice Matters**
- Now using original TSVC C functions as baseline
- C reference provides realistic CPU performance
- GPU vs CPU comparison shows true parallelization benefit

### 3. **Static Analysis Helps**
- 98% vs 95% pass rate with/without analysis
- Not all patterns need analysis
- Some edge cases still missed

### 4. **LLM Consistency**
- 17% need retries
- Some errors persist across attempts
- 5+5 reset helps but not always

---

## Future Work

### Short Term
1. **Fix s421 prompt**
   - Add explicit constexpr instruction
   - Provide working example
   - Test on similar patterns

2. **Run performance experiments**
   - Measure speedups vs C reference baseline
   - Compare against hand-written CUDA
   - Analyze kernel launch overhead impact

3. **Add more analysis modules**
   - Memory access pattern analysis
   - Register pressure prediction
   - Occupancy optimization

### Long Term
1. **Auto-tuning integration**
   - BLOCK_SIZE optimization
   - Grid size tuning
   - Memory layout optimization

2. **Performance optimization**
   - Beyond correctness â†’ optimal code
   - Memory coalescing hints
   - Shared memory utilization

3. **Broader benchmarks**
   - Beyond TSVC
   - Real-world kernels
   - Production workloads

4. **Model improvement**
   - Fine-tune on Triton corpus
   - Few-shot learning with examples
   - Chain-of-thought for complex patterns

---

# Thank You!

## Summary

**Infrastructure:**
- Automated TSVC â†’ Triton pipeline
- 8 static analysis modules
- 5+5 retry strategy
- Comprehensive testing

**Results (To Be Measured):**
- Correctness vs TSVC C reference
- First-try success rate
- Performance speedups (GPU vs CPU)

**Impact:**
- Demonstrates LLM capability for specialized code generation
- Shows value of static analysis integration
- Provides framework for future GPU kernel automation

---

## Questions?

ğŸ“§ Contact: qin-x18@mails.tsinghua.edu.cn
ğŸ”— Repository: [Add your repo link]
ğŸ“„ Paper: [In progress]

**Next steps:** Fix s421, improve baselines, expand to more benchmarks!
