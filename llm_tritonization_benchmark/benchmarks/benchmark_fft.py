"""
FFT Benchmark: Baseline (cuFFT) vs LLM Triton

NOTE: No expert Triton FFT implementation exists, so this is a 2-way comparison:
1. Baseline (PyTorch/cuFFT) - NVIDIA's highly optimized FFT library
2. LLM Triton - Generated by Claude from baseline

Expected result: cuFFT should be significantly faster
- cuFFT has decades of optimization
- Mixed-radix algorithms, architecture-specific tuning
- FFT requires log2(N) stages with complex memory patterns

To generate LLM Triton implementation:
    export ANTHROPIC_API_KEY=your_key
    cd /home/qinxiao/workspace/triton_performance_analysis/llm_tritonization_benchmark
    python utilities/generate_llm_triton.py
"""

import torch
import time
import sys
import os

# Add paths
sys.path.insert(0, "../baselines")
sys.path.insert(0, "../llm_triton")

from fft_baseline import fft_baseline

# Try to import LLM implementation
try:
    from fft_triton_llm import fft_triton as fft_llm
    HAS_LLM_IMPL = True
except ImportError:
    print("‚ö†Ô∏è  LLM Triton implementation not found!")
    print("    Generate it first using: python utilities/generate_llm_triton.py")
    print("    (Requires ANTHROPIC_API_KEY environment variable)")
    HAS_LLM_IMPL = False

# Set CUDA device
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

print("="*100)
print("FFT: 2-WAY COMPARISON (Baseline vs LLM Triton)")
print("="*100)
print("\nNOTE: No expert Triton implementation available")
print("      This benchmark compares PyTorch/cuFFT vs LLM-generated Triton\n")
print("-"*100)

# Test configurations - powers of 2 for FFT
sizes = [256, 512, 1024, 2048, 4096]
batch_size = 128

print(f"\n{'Size':<12} {'Baseline':<15} {'LLM':<15} {'Speedup':<15}")
print("-"*100)

results = []

for n in sizes:
    # Create complex input
    x_real = torch.randn(batch_size, n, device='cuda', dtype=torch.float32)
    x_imag = torch.randn(batch_size, n, device='cuda', dtype=torch.float32)
    x = torch.complex(x_real, x_imag)

    # Warmup
    for _ in range(5):
        _ = fft_baseline(x)
        if HAS_LLM_IMPL:
            try:
                _ = fft_llm(x)
            except:
                pass
    torch.cuda.synchronize()

    # Benchmark Baseline (cuFFT)
    start = time.perf_counter()
    for _ in range(50):
        y_baseline = fft_baseline(x)
    torch.cuda.synchronize()
    baseline_time = (time.perf_counter() - start) / 50 * 1000

    # Benchmark LLM
    if HAS_LLM_IMPL:
        try:
            start = time.perf_counter()
            for _ in range(50):
                y_llm = fft_llm(x)
            torch.cuda.synchronize()
            llm_time = (time.perf_counter() - start) / 50 * 1000

            # Verify correctness (relaxed tolerance for FFT)
            max_diff = torch.max(torch.abs(y_baseline - y_llm)).item()
            if max_diff > 1e-3:
                print(f"‚ö†Ô∏è  Warning: FFT results differ by {max_diff:.6f}")

            speedup = baseline_time / llm_time
            speedup_str = f"{speedup:.2f}x"

            results.append({
                'size': n,
                'baseline_time': baseline_time,
                'llm_time': llm_time,
                'speedup': speedup
            })

        except Exception as e:
            llm_time = float('inf')
            speedup_str = "FAILED"
            print(f"Size {n}: Error - {str(e)}")

    else:
        llm_time = 0.0
        speedup_str = "N/A"

    print(f"{n:<12} {baseline_time:>13.4f}ms {llm_time:>13.4f}ms {speedup_str:>13}")

print("="*100)

if HAS_LLM_IMPL and results:
    print("\nüìä SUMMARY")
    print("-"*100)

    import numpy as np
    avg_speedup = np.mean([r['speedup'] for r in results if r['speedup'] < 100])

    print(f"\nAverage LLM/Baseline ratio: {avg_speedup:.2f}x")

    if avg_speedup >= 1.0:
        print(f"‚úÖ SURPRISING - LLM Triton matches or beats cuFFT!")
    elif avg_speedup >= 0.5:
        print(f"‚úì  GOOD - LLM achieves {avg_speedup*100:.1f}% of cuFFT performance")
    elif avg_speedup >= 0.2:
        print(f"‚ö†Ô∏è  ACCEPTABLE - LLM achieves {avg_speedup*100:.1f}% of cuFFT (expected for complex algorithms)")
    else:
        print(f"‚ùå POOR - LLM significantly slower than cuFFT ({avg_speedup*100:.1f}%)")

    print("\nExpectations:")
    print("  - cuFFT is one of the most optimized libraries in existence")
    print("  - FFT requires log2(N) kernel launches with complex memory patterns")
    print("  - Even expert hand-written Triton FFT would struggle vs cuFFT")
    print("  - This test shows LLM limits on algorithm-heavy operations")

elif not HAS_LLM_IMPL:
    print("\n‚ö†Ô∏è  Could not run comparison - LLM implementation missing")
    print("\nTo generate LLM Triton FFT:")
    print("  1. Set ANTHROPIC_API_KEY environment variable")
    print("  2. Run: python utilities/generate_llm_triton.py")
    print("  3. Run this benchmark again")

print()
