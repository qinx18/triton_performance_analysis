# LLM-Generated Triton Implementation for s275
# Generated: 2025-11-17 17:31:35
# Model: claude-sonnet-4-20250514
# Stop Reason: end_turn
# Input Tokens: 631
# Output Tokens: 782

================================================================================
PROMPT:
================================================================================
I have a PyTorch/Python baseline implementation that I want to optimize using Triton for GPU acceleration.

Baseline Implementation:
```python
import torch

def s275_pytorch(aa, bb, cc):
    """
    PyTorch implementation of TSVC s275 - conditional forward recurrence.
    
    Original C code:
    for (int nl = 0; nl < 10*(iterations/LEN_2D); nl++) {
        for (int i = 0; i < LEN_2D; i++) {
            if (aa[0][i] > (real_t)0.) {
                for (int j = 1; j < LEN_2D; j++) {
                    aa[j][i] = aa[j-1][i] + bb[j][i] * cc[j][i];
                }
            }
        }
    }
    """
    aa = aa.contiguous()
    bb = bb.contiguous()
    cc = cc.contiguous()
    
    # Clone aa to avoid modifying the input
    result_aa = aa.clone()
    
    # Get dimensions
    len_2d = aa.shape[1]
    
    # For each column i
    for i in range(len_2d):
        # Check condition: aa[0][i] > 0
        if result_aa[0, i] > 0.0:
            # Forward recurrence for column i
            for j in range(1, len_2d):
                result_aa[j, i] = result_aa[j-1, i] + bb[j, i] * cc[j, i]
    
    return result_aa
```

Please generate a complete, optimized Triton implementation that:
1. Includes a @triton.jit kernel function named `s275_kernel`
2. Includes a Python wrapper function named `s275_triton` (NOT `s275_pytorch`)
3. The wrapper function must have THE EXACT SAME function signature as the baseline (same parameter names and order)
4. Uses appropriate block sizes and memory access patterns
5. Handles edge cases with masking
6. Is functionally equivalent to the baseline (same inputs, same outputs)
7. Returns the same type as the baseline (single tensor or tuple of tensors)
8. Includes brief comments explaining key optimizations

IMPORTANT:
- Wrapper function name: `s275_triton`
- Function signature must match the baseline exactly (same parameters in same order)
- Accept all parameters the baseline accepts, even if some are modified in-place or used only as outputs

Provide ONLY the Python code, no additional explanation.

================================================================================
RESPONSE:
================================================================================
```python
import torch
import triton
import triton.language as tl

@triton.jit
def s275_kernel(
    aa_ptr, bb_ptr, cc_ptr, result_ptr,
    len_2d,
    BLOCK_SIZE: tl.constexpr
):
    # Get column index
    col_idx = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    col_mask = col_idx < len_2d
    
    # Load first row to check condition
    first_row_offset = col_idx
    first_row_vals = tl.load(aa_ptr + first_row_offset, mask=col_mask, other=0.0)
    
    # Check condition: aa[0][col] > 0
    condition = first_row_vals > 0.0
    
    # Process each column that meets the condition
    for col in range(BLOCK_SIZE):
        if col_idx[col] < len_2d and condition[col]:
            # Initialize first element for this column
            prev_val = first_row_vals[col]
            tl.store(result_ptr + col_idx[col], prev_val, mask=col_idx[col] < len_2d)
            
            # Forward recurrence for this column
            for j in range(1, len_2d):
                bb_offset = j * len_2d + col_idx[col]
                cc_offset = j * len_2d + col_idx[col]
                result_offset = j * len_2d + col_idx[col]
                
                if col_idx[col] < len_2d:
                    bb_val = tl.load(bb_ptr + bb_offset)
                    cc_val = tl.load(cc_ptr + cc_offset)
                    new_val = prev_val + bb_val * cc_val
                    tl.store(result_ptr + result_offset, new_val)
                    prev_val = new_val

def s275_triton(aa, bb, cc):
    """
    Triton implementation of TSVC s275 - conditional forward recurrence.
    Optimized for GPU execution with column-parallel processing.
    """
    aa = aa.contiguous()
    bb = bb.contiguous() 
    cc = cc.contiguous()
    
    # Clone aa to avoid modifying input and initialize result
    result_aa = aa.clone()
    
    # Get dimensions
    len_2d = aa.shape[1]
    
    # Handle empty case
    if len_2d == 0:
        return result_aa
    
    # Use smaller block size for better load balancing with conditional execution
    BLOCK_SIZE = 32
    grid = (triton.cdiv(len_2d, BLOCK_SIZE),)
    
    # Launch kernel - processes columns in parallel
    # Each thread block handles multiple columns and performs sequential recurrence
    s275_kernel[grid](
        aa, bb, cc, result_aa,
        len_2d,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return result_aa
```
