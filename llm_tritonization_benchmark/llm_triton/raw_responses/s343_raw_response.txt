# LLM-Generated Triton Implementation for s343
# Generated: 2025-11-17 17:40:51
# Model: claude-sonnet-4-20250514
# Stop Reason: end_turn
# Input Tokens: 685
# Output Tokens: 1177

================================================================================
PROMPT:
================================================================================
I have a PyTorch/Python baseline implementation that I want to optimize using Triton for GPU acceleration.

Baseline Implementation:
```python
import torch

def s343_pytorch(aa, bb, flat_2d_array):
    """
    PyTorch implementation of TSVC s343 - conditional array packing.
    
    Original C code:
    for (int nl = 0; nl < 10*(iterations/LEN_2D); nl++) {
        k = -1;
        for (int i = 0; i < LEN_2D; i++) {
            for (int j = 0; j < LEN_2D; j++) {
                if (bb[j][i] > (real_t)0.) {
                    k++;
                    flat_2d_array[k] = aa[j][i];
                }
            }
        }
    }
    
    Args:
        aa: 2D tensor (read-only)
        bb: 2D tensor (read-only) 
        flat_2d_array: 1D tensor (read-write)
    
    Returns:
        flat_2d_array: Modified 1D tensor
    """
    aa = aa.contiguous()
    bb = bb.contiguous()
    flat_2d_array = flat_2d_array.contiguous()
    
    LEN_2D = aa.shape[0]
    
    # Create mask for bb > 0
    mask = bb > 0.0
    
    # Extract elements from aa where bb > 0, column-major order (j, i indexing)
    selected_elements = aa[mask]
    
    # Update flat_2d_array with selected elements
    num_selected = selected_elements.shape[0]
    flat_2d_array[:num_selected] = selected_elements
    
    return flat_2d_array
```

Please generate a complete, optimized Triton implementation that:
1. Includes a @triton.jit kernel function named `s343_kernel`
2. Includes a Python wrapper function named `s343_triton` (NOT `s343_pytorch`)
3. The wrapper function must have THE EXACT SAME function signature as the baseline (same parameter names and order)
4. Uses appropriate block sizes and memory access patterns
5. Handles edge cases with masking
6. Is functionally equivalent to the baseline (same inputs, same outputs)
7. Returns the same type as the baseline (single tensor or tuple of tensors)
8. Includes brief comments explaining key optimizations

IMPORTANT:
- Wrapper function name: `s343_triton`
- Function signature must match the baseline exactly (same parameters in same order)
- Accept all parameters the baseline accepts, even if some are modified in-place or used only as outputs

Provide ONLY the Python code, no additional explanation.

================================================================================
RESPONSE:
================================================================================
```python
import torch
import triton
import triton.language as tl

@triton.jit
def s343_kernel(
    aa_ptr, bb_ptr, flat_2d_array_ptr, output_count_ptr,
    LEN_2D, total_elements,
    BLOCK_SIZE: tl.constexpr
):
    """
    Triton kernel for conditional array packing.
    Uses atomic operations to handle race conditions in output indexing.
    """
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # Mask for valid elements
    mask = offsets < total_elements
    
    # Load bb values and create condition mask
    bb_vals = tl.load(bb_ptr + offsets, mask=mask, other=0.0)
    condition_mask = (bb_vals > 0.0) & mask
    
    # Load aa values where condition is true
    aa_vals = tl.load(aa_ptr + offsets, mask=condition_mask, other=0.0)
    
    # For each valid element that meets the condition, atomically get output index
    for i in range(BLOCK_SIZE):
        if block_start + i < total_elements and condition_mask[i]:
            # Atomically increment counter and get index
            output_idx = tl.atomic_add(output_count_ptr, 1)
            # Store the value at the obtained index
            tl.store(flat_2d_array_ptr + output_idx, aa_vals[i])

@triton.jit
def s343_kernel_optimized(
    aa_ptr, bb_ptr, flat_2d_array_ptr, valid_indices_ptr, 
    LEN_2D, num_valid,
    BLOCK_SIZE: tl.constexpr
):
    """
    Optimized kernel that works with pre-computed valid indices.
    Avoids atomic operations by using predetermined output positions.
    """
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # Mask for valid outputs
    mask = offsets < num_valid
    
    # Load the source indices
    src_indices = tl.load(valid_indices_ptr + offsets, mask=mask, other=0)
    
    # Load aa values from the valid source positions
    aa_vals = tl.load(aa_ptr + src_indices, mask=mask, other=0.0)
    
    # Store to consecutive positions in output
    tl.store(flat_2d_array_ptr + offsets, aa_vals, mask=mask)

def s343_triton(aa, bb, flat_2d_array):
    """
    Triton implementation of TSVC s343 - conditional array packing.
    Uses a two-pass approach: first find valid indices, then pack elements.
    
    Args:
        aa: 2D tensor (read-only)
        bb: 2D tensor (read-only) 
        flat_2d_array: 1D tensor (read-write)
    
    Returns:
        flat_2d_array: Modified 1D tensor
    """
    aa = aa.contiguous()
    bb = bb.contiguous()
    flat_2d_array = flat_2d_array.contiguous()
    
    LEN_2D = aa.shape[0]
    total_elements = aa.numel()
    
    # First pass: find valid indices using PyTorch (more efficient for sparse selection)
    mask = bb > 0.0
    valid_indices = torch.nonzero(mask, as_tuple=False).view(-1)
    
    # Convert 2D indices to flat indices (column-major order to match original)
    flat_indices = valid_indices // LEN_2D + (valid_indices % LEN_2D) * LEN_2D
    num_valid = valid_indices.shape[0]
    
    if num_valid == 0:
        return flat_2d_array
    
    # Second pass: use Triton to efficiently pack the elements
    BLOCK_SIZE = 256
    grid = (triton.cdiv(num_valid, BLOCK_SIZE),)
    
    s343_kernel_optimized[grid](
        aa.data_ptr(),
        bb.data_ptr(),
        flat_2d_array.data_ptr(),
        flat_indices.data_ptr(),
        LEN_2D,
        num_valid,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return flat_2d_array
```
