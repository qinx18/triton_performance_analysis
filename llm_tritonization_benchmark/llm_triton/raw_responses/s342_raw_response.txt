# LLM-Generated Triton Implementation for s342
# Generated: 2025-11-17 17:40:29
# Model: claude-sonnet-4-20250514
# Stop Reason: end_turn
# Input Tokens: 544
# Output Tokens: 1205

================================================================================
PROMPT:
================================================================================
I have a PyTorch/Python baseline implementation that I want to optimize using Triton for GPU acceleration.

Baseline Implementation:
```python
import torch

def s342_pytorch(a, b):
    """
    PyTorch implementation of TSVC s342 - conditional assignment with compression.
    
    Original C code:
    for (int nl = 0; nl < iterations; nl++) {
        j = -1;
        for (int i = 0; i < LEN_1D; i++) {
            if (a[i] > (real_t)0.) {
                j++;
                a[i] = b[j];
            }
        }
    }
    
    Arrays: a (rw), b (r)
    """
    a = a.contiguous()
    b = b.contiguous()
    
    # Find indices where a > 0
    mask = a > 0.0
    
    # Count how many elements satisfy the condition
    num_positive = torch.sum(mask).item()
    
    if num_positive > 0:
        # Get the first num_positive elements from b
        b_subset = b[:num_positive]
        
        # Assign b values to positions where a > 0
        a[mask] = b_subset
    
    return a
```

Please generate a complete, optimized Triton implementation that:
1. Includes a @triton.jit kernel function named `s342_kernel`
2. Includes a Python wrapper function named `s342_triton` (NOT `s342_pytorch`)
3. The wrapper function must have THE EXACT SAME function signature as the baseline (same parameter names and order)
4. Uses appropriate block sizes and memory access patterns
5. Handles edge cases with masking
6. Is functionally equivalent to the baseline (same inputs, same outputs)
7. Returns the same type as the baseline (single tensor or tuple of tensors)
8. Includes brief comments explaining key optimizations

IMPORTANT:
- Wrapper function name: `s342_triton`
- Function signature must match the baseline exactly (same parameters in same order)
- Accept all parameters the baseline accepts, even if some are modified in-place or used only as outputs

Provide ONLY the Python code, no additional explanation.

================================================================================
RESPONSE:
================================================================================
```python
import torch
import triton
import triton.language as tl

@triton.jit
def s342_kernel(
    a_ptr, b_ptr, output_ptr,
    mask_ptr, prefix_sum_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Triton kernel for s342 - conditional assignment with compression.
    Uses prefix sum to determine correct indices for compression.
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # Boundary check
    mask_boundary = offsets < n_elements
    
    # Load current values and compute condition mask
    a_vals = tl.load(a_ptr + offsets, mask=mask_boundary, other=0.0)
    condition_mask = a_vals > 0.0
    
    # Load precomputed mask and prefix sum
    mask_vals = tl.load(mask_ptr + offsets, mask=mask_boundary, other=0)
    prefix_vals = tl.load(prefix_sum_ptr + offsets, mask=mask_boundary, other=0)
    
    # For elements that satisfy condition, load corresponding b values
    b_indices = prefix_vals - 1  # Convert to 0-based indexing
    b_vals = tl.load(b_ptr + b_indices, mask=(mask_boundary & condition_mask), other=0.0)
    
    # Select between original a values and new b values based on condition
    result = tl.where(condition_mask, b_vals, a_vals)
    
    # Store result
    tl.store(output_ptr + offsets, result, mask=mask_boundary)

@triton.jit
def prefix_sum_kernel(
    input_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Simple prefix sum kernel for small arrays.
    For larger arrays, would need a more sophisticated implementation.
    """
    pid = tl.program_id(axis=0)
    
    if pid == 0:
        # Load all elements (assuming small enough to fit in one block)
        offsets = tl.arange(0, BLOCK_SIZE)
        mask = offsets < n_elements
        
        vals = tl.load(input_ptr + offsets, mask=mask, other=0)
        
        # Compute prefix sum using scan
        for stride in range(1, BLOCK_SIZE, 1):
            if stride >= n_elements:
                break
            shifted_vals = tl.zeros_like(vals)
            shift_mask = offsets >= stride
            shift_offsets = offsets - stride
            shift_vals = tl.load(input_ptr + shift_offsets, mask=(mask & shift_mask), other=0)
            vals = vals + tl.where(shift_mask, shift_vals, 0)
        
        tl.store(output_ptr + offsets, vals, mask=mask)

def s342_triton(a, b):
    """
    Triton implementation of TSVC s342 - conditional assignment with compression.
    Uses prefix sum to handle compression efficiently on GPU.
    """
    a = a.contiguous()
    b = b.contiguous()
    n_elements = a.shape[0]
    
    # Create mask for condition a > 0
    mask = (a > 0.0).to(torch.int32)
    
    # Compute prefix sum for compression indices
    prefix_sum = torch.zeros_like(mask)
    
    # Use simple approach for prefix sum (for small arrays)
    # For larger arrays, would use a more efficient prefix sum implementation
    BLOCK_SIZE = min(1024, triton.next_power_of_2(n_elements))
    
    if n_elements <= BLOCK_SIZE:
        # Use Triton prefix sum for small arrays
        prefix_sum_kernel[(1,)](
            mask, prefix_sum,
            n_elements,
            BLOCK_SIZE=BLOCK_SIZE
        )
    else:
        # Fallback to PyTorch for larger arrays
        prefix_sum = torch.cumsum(mask, dim=0)
    
    # Create output tensor
    output = torch.empty_like(a)
    
    # Launch main kernel
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    
    s342_kernel[grid](
        a, b, output,
        mask, prefix_sum,
        n_elements,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    
    # Copy result back to original tensor (to match in-place behavior)
    a.copy_(output)
    return a
```
