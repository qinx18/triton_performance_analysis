# LLM-Generated Triton Implementation for covariance (Polybench)
# Generated: 2026-02-09 18:47:10
# Model: claude-sonnet-4-20250514

================================================================================
PROMPT:
================================================================================
I have a Polybench/C kernel that I want to implement in Triton for GPU acceleration.

## Original C Code:
```c
#include <math.h>

#define M 80
#define N 100

float mean[M];
float data[N][M];
float cov[M][M];

float float_n = 100.0f;

void covariance_kernel() {
  int i, j, k;
#pragma scop
  for (j = 0; j < M; j++)
    {
      mean[j] = 0.0;
      for (i = 0; i < N; i++)
        mean[j] += data[i][j];
      mean[j] /= float_n;
    }

  for (i = 0; i < N; i++)
    for (j = 0; j < M; j++)
      data[i][j] -= mean[j];

  for (i = 0; i < M; i++)
    for (j = i; j < M; j++)
      {
        cov[i][j] = 0.0;
        for (k = 0; k < N; k++)
	  cov[i][j] += data[k][i] * data[k][j];
        cov[i][j] /= (float_n - 1.0);
        cov[j][i] = cov[i][j];
      }
#pragma endscop
}

```

## Kernel Loop to Implement:
```c
for (j = 0; j < M; j++)
    {
      mean[j] = 0.0;
      for (i = 0; i < N; i++)
        mean[j] += data[i][j];
      mean[j] /= float_n;
    }

  for (i = 0; i < N; i++)
    for (j = 0; j < M; j++)
      data[i][j] -= mean[j];

  for (i = 0; i < M; i++)
    for (j = i; j < M; j++)
      {
        cov[i][j] = 0.0;
        for (k = 0; k < N; k++)
	  cov[i][j] += data[k][i] * data[k][j];
        cov[i][j] /= (float_n - 1.0);
        cov[j][i] = cov[i][j];
      }
```


## Array Information:
- `cov`: read-write
- `data`: read-write
- `mean`: read-write

## Dimension Parameters (compile-time constants in C, runtime parameters in Triton):
- `M` = 80
- `N` = 100

## Requirements:
Please generate a complete Triton implementation that:
1. Includes a @triton.jit kernel function named `covariance_kernel`
2. Includes a Python wrapper function named `covariance_triton`
3. The wrapper accepts tensor arrays, scalar parameters, and dimension parameters
4. Uses appropriate block sizes and memory access patterns
5. Handles edge cases with masking
6. Is functionally equivalent to the C code (same computation, same results)
7. For 2D arrays, compute linear index as `row * stride + col`
8. For 3D arrays, compute linear index as `dim0 * (dim1_size * dim2_size) + dim1 * dim2_size + dim2`

## REQUIRED function signature (use EXACTLY these parameter names):
```python
def covariance_triton(cov, data, mean, float_n, M, N):
    ...  # kernel computation
```

## CRITICAL: Triton Compilation Rules

**NEVER use `tl.arange()` inside a for loop:**
```python
# WRONG
for block_start in range(0, n, BLOCK_SIZE):
    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # ERROR!

# CORRECT
offsets = tl.arange(0, BLOCK_SIZE)  # Define once at start
for block_start in range(0, n, BLOCK_SIZE):
    current_offsets = block_start + offsets
```

**NEVER use scalar indexing inside @triton.jit kernel:**
```python
# WRONG
for i in range(BLOCK_SIZE):
    val = tensor[i]

# CORRECT
mask = offsets < n_elements
vals = tl.load(ptr + offsets, mask=mask)
```

**NEVER use non-existent Triton functions:**
- Use Python operators: `a * b`, `a / b`, `a + b` (not `tl.mul`, `tl.div`, `tl.add`)
- Use `triton.cdiv()` in wrapper only

**NEVER use Python lists, break/continue inside @triton.jit kernels**
**Pass tensors directly to kernels, NOT data_ptr()**
**NEVER use chained comparisons (use separate comparisons with &)**

Provide ONLY the Python code, no additional explanation.

================================================================================
RESPONSE:
================================================================================
```python
import triton
import triton.language as tl
import torch

@triton.jit
def covariance_kernel(
    cov_ptr, data_ptr, mean_ptr,
    float_n, M, N,
    BLOCK_SIZE: tl.constexpr
):
    # Step 1: Compute mean for each column j
    j_offsets = tl.arange(0, BLOCK_SIZE)
    
    for j_block_start in range(0, M, BLOCK_SIZE):
        j_current = j_block_start + j_offsets
        j_mask = j_current < M
        
        # Initialize mean values to 0
        mean_vals = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        
        # Sum over all rows for each column
        for i in range(N):
            data_indices = i * M + j_current
            data_vals = tl.load(data_ptr + data_indices, mask=j_mask, other=0.0)
            mean_vals += data_vals
        
        # Divide by N to get mean
        mean_vals = mean_vals / float_n
        
        # Store mean values
        tl.store(mean_ptr + j_current, mean_vals, mask=j_mask)
    
    # Step 2: Subtract mean from data
    for i in range(N):
        for j_block_start in range(0, M, BLOCK_SIZE):
            j_current = j_block_start + j_offsets
            j_mask = j_current < M
            
            data_indices = i * M + j_current
            data_vals = tl.load(data_ptr + data_indices, mask=j_mask, other=0.0)
            mean_vals = tl.load(mean_ptr + j_current, mask=j_mask, other=0.0)
            
            centered_vals = data_vals - mean_vals
            tl.store(data_ptr + data_indices, centered_vals, mask=j_mask)
    
    # Step 3: Compute covariance matrix (upper triangular)
    for i in range(M):
        for j_block_start in range(i, M, BLOCK_SIZE):
            j_current = j_block_start + j_offsets
            j_mask = (j_current < M) & (j_current >= i)
            
            # Initialize covariance values
            cov_vals = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
            
            # Sum products over all samples
            for k in range(N):
                data_i_vals = tl.full([BLOCK_SIZE], 0.0, dtype=tl.float32)
                data_i_val = tl.load(data_ptr + k * M + i)
                data_i_vals = data_i_val
                
                data_j_vals = tl.load(data_ptr + k * M + j_current, mask=j_mask, other=0.0)
                
                cov_vals += data_i_vals * data_j_vals
            
            # Divide by (N-1)
            cov_vals = cov_vals / (float_n - 1.0)
            
            # Store upper triangular values
            cov_indices = i * M + j_current
            tl.store(cov_ptr + cov_indices, cov_vals, mask=j_mask)
            
            # Store symmetric values (lower triangular)
            valid_j_mask = j_mask & (j_current != i)  # Don't duplicate diagonal
            if tl.sum(valid_j_mask.to(tl.int32)) > 0:
                sym_indices = j_current * M + i
                tl.store(cov_ptr + sym_indices, cov_vals, mask=valid_j_mask)

def covariance_triton(cov, data, mean, float_n, M, N):
    BLOCK_SIZE = 32
    
    # Launch kernel with single thread block since we handle blocking internally
    grid = (1,)
    
    covariance_kernel[grid](
        cov, data, mean,
        float_n, M, N,
        BLOCK_SIZE=BLOCK_SIZE
    )
```
