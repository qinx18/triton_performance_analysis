# LLM-Generated Triton Implementation for floyd_warshall (Polybench)
# Generated: 2026-02-09 18:51:35
# Model: claude-sonnet-4-20250514

================================================================================
PROMPT:
================================================================================
I have a Polybench/C kernel that I want to implement in Triton for GPU acceleration.

## Original C Code:
```c
#include <math.h>

#define N 120

float path[N][N];

void floyd_warshall_kernel() {
  int i, j, k;
#pragma scop
  for (k = 0; k < N; k++)
    {
      for(i = 0; i < N; i++)
	for (j = 0; j < N; j++)
	  path[i][j] = path[i][j] < path[i][k] + path[k][j] ?
	    path[i][j] : path[i][k] + path[k][j];
    }
#pragma endscop
}

```

## Kernel Loop to Implement:
```c
for (k = 0; k < N; k++)
    {
      for(i = 0; i < N; i++)
	for (j = 0; j < N; j++)
	  path[i][j] = path[i][j] < path[i][k] + path[k][j] ?
	    path[i][j] : path[i][k] + path[k][j];
    }
```


## Array Information:
- `path`: read-write

## Dimension Parameters (compile-time constants in C, runtime parameters in Triton):
- `N` = 120

## Requirements:
Please generate a complete Triton implementation that:
1. Includes a @triton.jit kernel function named `floyd_warshall_kernel`
2. Includes a Python wrapper function named `floyd_warshall_triton`
3. The wrapper accepts tensor arrays, scalar parameters, and dimension parameters
4. Uses appropriate block sizes and memory access patterns
5. Handles edge cases with masking
6. Is functionally equivalent to the C code (same computation, same results)
7. For 2D arrays, compute linear index as `row * stride + col`
8. For 3D arrays, compute linear index as `dim0 * (dim1_size * dim2_size) + dim1 * dim2_size + dim2`

## REQUIRED function signature (use EXACTLY these parameter names):
```python
def floyd_warshall_triton(path, N):
    ...  # kernel computation
```

## CRITICAL: Triton Compilation Rules

**NEVER use `tl.arange()` inside a for loop:**
```python
# WRONG
for block_start in range(0, n, BLOCK_SIZE):
    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # ERROR!

# CORRECT
offsets = tl.arange(0, BLOCK_SIZE)  # Define once at start
for block_start in range(0, n, BLOCK_SIZE):
    current_offsets = block_start + offsets
```

**NEVER use scalar indexing inside @triton.jit kernel:**
```python
# WRONG
for i in range(BLOCK_SIZE):
    val = tensor[i]

# CORRECT
mask = offsets < n_elements
vals = tl.load(ptr + offsets, mask=mask)
```

**NEVER use non-existent Triton functions:**
- Use Python operators: `a * b`, `a / b`, `a + b` (not `tl.mul`, `tl.div`, `tl.add`)
- Use `triton.cdiv()` in wrapper only

**NEVER use Python lists, break/continue inside @triton.jit kernels**
**Pass tensors directly to kernels, NOT data_ptr()**
**NEVER use chained comparisons (use separate comparisons with &)**

Provide ONLY the Python code, no additional explanation.

================================================================================
RESPONSE:
================================================================================
```python
import triton
import triton.language as tl
import torch

@triton.jit
def floyd_warshall_kernel(path_ptr, N, BLOCK_SIZE: tl.constexpr):
    # Get block indices
    block_i = tl.program_id(0)
    block_j = tl.program_id(1)
    
    # Compute base offsets once
    i_offsets = tl.arange(0, BLOCK_SIZE)
    j_offsets = tl.arange(0, BLOCK_SIZE)
    
    # Compute actual indices
    i_indices = block_i * BLOCK_SIZE + i_offsets
    j_indices = block_j * BLOCK_SIZE + j_offsets
    
    # Create masks
    i_mask = i_indices < N
    j_mask = j_indices < N
    
    # Sequential k loop (cannot be parallelized due to dependencies)
    for k in range(N):
        # Load path[i][j] values
        ij_offsets = i_indices[:, None] * N + j_indices[None, :]
        ij_mask = i_mask[:, None] & j_mask[None, :]
        path_ij = tl.load(path_ptr + ij_offsets, mask=ij_mask)
        
        # Load path[i][k] values
        ik_offsets = i_indices * N + k
        ik_mask = i_mask & (k < N)
        path_ik = tl.load(path_ptr + ik_offsets, mask=ik_mask)
        
        # Load path[k][j] values
        kj_offsets = k * N + j_indices
        kj_mask = j_mask & (k < N)
        path_kj = tl.load(path_ptr + kj_offsets, mask=kj_mask)
        
        # Compute new distances
        new_dist = path_ik[:, None] + path_kj[None, :]
        
        # Update path[i][j] with minimum
        updated_path = tl.minimum(path_ij, new_dist)
        
        # Store back
        tl.store(path_ptr + ij_offsets, updated_path, mask=ij_mask)

def floyd_warshall_triton(path, N):
    BLOCK_SIZE = 16
    
    # Calculate grid dimensions
    grid_dim = triton.cdiv(N, BLOCK_SIZE)
    grid = (grid_dim, grid_dim)
    
    # Launch kernel
    floyd_warshall_kernel[grid](
        path, N, BLOCK_SIZE
    )
```
